<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>AV-DAR</title>
  <link rel="icon" type="image/x-icon" href="static/images/icon.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="static/css/idemo.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <style>
    .demo-placeholder {
      position: relative;
      overflow: hidden;
      cursor: pointer;
      border-radius: 0.5rem;
    }

    .demo-placeholder .demo-img {
      display: block;
      width: 100%;
      height: auto;
      transition: transform 0.3s ease;
    }

    .demo-placeholder .overlay {
      position: absolute;
      top: 0; left: 0;
      width: 100%; height: 100%;
      background: rgba(0, 0, 0, 0.4);
      display: flex;
      align-items: center;
      justify-content: center;
      opacity: 0;
      transition: opacity 0.3s ease;
    }

    .demo-placeholder .overlay-content {
      text-align: center;
      color: #fff;
    }

    .demo-placeholder .overlay-text {
      margin-top: 0.5rem;
      font-size: 1.1rem;
    }
    .demo-placeholder:hover .overlay {
      opacity: 1;
    }
    .demo-placeholder:hover .demo-img {
      transform: scale(1.05);
    }
  </style>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  
  <!-- Three.js from CDN -->
  <!-- <script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r123/three.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/three@0.123.0/examples/js/loaders/OBJLoader.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/three@0.123.0/examples/js/loaders/MTLLoader.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/three@0.123.0/examples/js/controls/OrbitControls.js"></script>

  <script src="https://cdn.jsdelivr.net/npm/chart.js"></script> Chart.js library -->

  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>

  
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Differentiable Room Acoustic Rendering with Multi-View Vision Priors</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://humathe.github.io/" target="_blank">Derong Jin</a>,</span>
                  <span class="author-block">
                    <a href="https://ruohangao.github.io" target="_blank">Ruohan Gao</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">University of Maryland, College Park<br>Arxiv 2025</span>
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2504.21847.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2504.21847" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>


              <span class="link-block">
                <a href="idemo.html" target="_blank" class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fas fa-play"></i>
                </span>
                <span>Demo</span>
                </a>
              </span>
              <!-- Github link -->
               <span class="link-block">
                <!-- <a href="https://github.com/YOUR REPO HERE" target="_blank"
                class="external-link button is-normal is-rounded is-dark"> -->
                <a target="_blank"
                class="external-link button is-normal is-rounded is-light">
                <span class="icon">
                  <i class="fab fa-github"></i>
                </span>
                <span>Code</span>
              </a>
            </span>

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- <video poster="" id="video-01" controls height="100%">
        <source src="static/videos/teaser_fast.mp4" type="video/mp4">
      </video> -->
      <div class="publication-video">
        <iframe src="https://www.youtube.com/embed/thtuB1kRwPU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
      </div>
      <h2 class="subtitle has-text-centered">
        Binaural audio tour rendered by AV-DAR across 6 real scenes. Trained on 1% (RAF) / 12 (HAA) IRs. Headphones are strongly recommended.
        <!-- Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus.  -->
      </h2>
      <h3 class="has-text-centered">
        Sound Source: <a href="https://pixabay.com/music/build-up-scenes-percussive-drums-131490/" target="_blank">DayNightMorning</a>
      </h3>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            <!-- Source:  -->
            <!-- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Proin ullamcorper tellus sed ante aliquam tempus. Etiam porttitor urna feugiat nibh elementum, et tempor dolor mattis. Donec accumsan enim augue, a vulputate nisi sodales sit amet. Proin bibendum ex eget mauris cursus euismod nec et nibh. Maecenas ac gravida ante, nec cursus dui. Vivamus purus nibh, placerat ac purus eget, sagittis vestibulum metus. Sed vestibulum bibendum lectus gravida commodo. Pellentesque auctor leo vitae sagittis suscipit. -->
            An immersive acoustic experience enabled by spatial audio is just as crucial as the visual aspect in creating realistic virtual environments. 
            However, existing methods for room impulse response estimation rely either on data-demanding learning-based models or computationally expensive physics-based modeling. 
            In this work, we introduce <b>Audio-Visual Differentiable Room Acoustic Rendering (AV-DAR)</b>, a framework that leverages visual cues extracted from multi-view images and acoustic beam tracing for physics-based room acoustic rendering. Experiments across six real-world environments from two datasets demonstrate that our multimodal, physics-based approach is efficient, interpretable, and accurate, significantly outperforming a series of prior methods. Notably, on the Real Acoustic Field dataset, AV-DAR achieves comparable performance to models trained on 10 times more data while delivering relative gains ranging from <b>16.6%</b> to <b>50.9%</b> when trained at the same scale.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Method Pipeline-->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Method Pipeline</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <figure class="image">
            <img src="static/images/pipeline.png" alt="Method Pipeline">
            <h2 class="subtitle has-text-justified">
              In the <b>top row</b>, we extract material-aware <span style="color:cornflowerblue">surface features</span> from multi-view images, 
              guiding reflection modeling. In the bottom row, we first compute the <span style="color:brown">reflection response</span> by combining <span style="color:cornflowerblue">surface features</span> with beam tracing (<b>left</b>), 
              and then integrate <span style="color: brown">residual components</span> by treating each surface point as a secondary source (<b>right</b>). The entire pipeline is differentiable, enabling 
              end-to-end optimization.
              <!-- Our method realistically simulates room acoustics by integrating visual analysis and acoustic modeling. We first process multi-view images to extract visual features that capture material and surface properties. These visual insights guide acoustic simulations, accurately modeling sound reflections and complex reverberation effects. The entire pipeline is differentiable, allowing end-to-end optimization for enhanced acoustic realism. -->
              <!-- Our framework contains two main components for rendering the room impulse response (RIR): <b>(1) Visual Processing (top):</b> Multi-view images of the scene are passed through a pre-trained vision encoder
              to extract pixel-aligned features at sampled points on the room surface. We then apply cross-attention both across views for each sampled point and across sampled points of query x
              to obtain a unified, material-aware visual feature $\mathrm{F}(\mathbf{x})$ (detailed in Section~\ref{sec:vision}). \textbf{(2) Acoustic Processing (bottom):} On the left, we illustrate our acoustic beam tracing procedure (Section~\ref{sec:beamtracing}), where we sample specular paths and compute the path reflection response, conditioned on both the positional encoding (Section~\ref{sec:mip-rr}) and the visual feature $\mathrm{F}({\bf x})$. On the right, we show how we model the residual acoustic field (Section~\ref{sec:residual}) by treating every point on the surface as a secondary sound source and integrating its contribution via Monte-Carlo integration. The entire pipeline is fully differentiable, enabling end-to-end optimization of both acoustic and visual parameters. -->
              <!-- <math>aa</math> -->
            </h2>
          </figure>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End method -->

<!-- Video carousel -->
<!-- <section class="hero is-small is-light"> -->
  <!-- <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Interactive Demo
        <a href="idemo.html" class="button is-light">
          <span class="icon">
            <i class="fas fa-play"></i>
          </span>
          <span>Explore the reflection response yourself!</span>
        </a>
      </h2> -->
      
      <!-- <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="demo-wrapper">
          <div id="idemo"></div>
          <div id="idemo-panel">
            <h3 class="idemo-subtitle"><b>Reflection Response Values</b>
              
              (Amplitude v.s. Frequency)
            </h3>
            
            <div id="chartContainer">
              <canvas id="chartCanvasOurs"></canvas>
            </div>
            <br>
            <div id="chartContainer">
              <canvas id="chartCanvasWoVision"></canvas>
            </div>
          </div>
          <div id="menu">
            <button id="mtl0">Texture</button>
            <button id="mtl-vis">RR: Ours</button>
            <button id="mtl-nov">RR: w/o Vision</button>
          </div>
        </div>

        <div class="demo-instructions">
          <h3>How to Use This Demo</h3>
          <ol>
            <li>
              <strong>Navigate the Scene:</strong>
              <ul>
                <li><kbd>Left-click + drag</kbd> to rotate your view.</li>
                <li><kbd>Right-click + drag</kbd> to pan.</li>
                <li>Scroll the mouse wheel to zoom in and out.</li>
              </ul>
            </li>
            <li>
              <strong>Inspect a Point's Frequency Response:</strong>
              <ul>
                <li>Click any red sphere on the room surface.</li>
                <li>The two plots on the right show amplitude vs. frequency for that point:</li>
                <ul>
                  <li><span style="color:rgb(54, 162, 235)">Blue curve</span> = Ours model.</li>
                  <li><span style="color:rgb(255, 99, 132)">Red curve</span> = Ours model without vision input.</li>
                </ul>
              </ul>
            </li>
            <li>
              <strong>Visualize the Whole-Room Frequency Response:</strong>
              <p>Use the buttons below the scene:</p>
              <table class="demo-controls-table">
                <thead>
                  <tr>
                    <th>Button</th>
                    <th>Description</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td><kbd>Texture</kbd></td>
                    <td>Show the room texture.</td>
                  </tr>
                  <tr>
                    <td><kbd>RR: Ours</kbd></td>
                    <td>Show the reflection response map predicted by our full model.</td>
                  </tr>
                  <tr>
                    <td><kbd>RR: w/o Vision</kbd></td>
                    <td>Same reflection response map but without using vision input (set vision feature to zero).</td>
                  </tr>
                </tbody>
              </table>
            </li>
          </ol>
          <p><em>Tip:</em> Hard, smooth materials tend to reflect high frequencies (red); soft, rough materials reflect low frequencies (blue).</p>
        </div>
      </div>
      </div> -->
    <!-- </div> -->
  <!-- </div> -->
<!-- </section> -->
<!-- End video carousel -->

<section class="hero is-small is-light demo-placeholder-section">
  <div class="hero-body">
  <div class="container">
    <h2 class="title is-3">Interactive Demo </h2>
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="demo-placeholder" onclick="window.open('idemo.html','_blank')">
          <img
            src="static/images/idemo/teaser.png"
            alt="Interactive demo preview"
            class="demo-img"
          />
          <div class="overlay">
            <div class="overlay-content">
              <span class="icon is-large">
                <i class="fas fa-play fa-2x"></i>
              </span>
              <p class="overlay-text">Click to explore the interactive demo</p>
            </div>
          </div>
        </div>
      </div>
    </div>
    </div>
  </div>
</section>


<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Qualitative Results</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <figure class="image">
            <img src="static/images/SignalComp.png" alt="SignalDistVisualization">
            <h2 class="subtitle has-text-justified">
              Signal spatial distribution visualization. <b>Top two rows:</b> Phase and amplitude maps at 0.6m wavelength. <b>Bottom row:</b> Loudness heatmap. Our model, trained on only 0.1% of the data, accurately captures source directivity and localization, yielding plausible phase and amplitude distributions, while baseline methods fail to reproduce these patterns even with 10× training data.
              <!-- Our method realistically simulates room acoustics by integrating visual analysis and acoustic modeling. We first process multi-view images to extract visual features that capture material and surface properties. These visual insights guide acoustic simulations, accurately modeling sound reflections and complex reverberation effects. The entire pipeline is differentiable, allowing end-to-end optimization for enhanced acoustic realism. -->
              <!-- Our framework contains two main components for rendering the room impulse response (RIR): <b>(1) Visual Processing (top):</b> Multi-view images of the scene are passed through a pre-trained vision encoder
              to extract pixel-aligned features at sampled points on the room surface. We then apply cross-attention both across views for each sampled point and across sampled points of query x
              to obtain a unified, material-aware visual feature $\mathrm{F}(\mathbf{x})$ (detailed in Section~\ref{sec:vision}). \textbf{(2) Acoustic Processing (bottom):} On the left, we illustrate our acoustic beam tracing procedure (Section~\ref{sec:beamtracing}), where we sample specular paths and compute the path reflection response, conditioned on both the positional encoding (Section~\ref{sec:mip-rr}) and the visual feature $\mathrm{F}({\bf x})$. On the right, we show how we model the residual acoustic field (Section~\ref{sec:residual}) by treating every point on the surface as a secondary sound source and integrating its contribution via Monte-Carlo integration. The entire pipeline is fully differentiable, enabling end-to-end optimization of both acoustic and visual parameters. -->
              <!-- <math>aa</math> -->
            </h2>
          </figure>
        </div>
      </div>

      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <figure class="image">
            <!-- <img src="static/images/reflection.png" alt="ReflectionResponseVisualization"> -->
            <img src="static/images/reflection-1.png" alt="ReflectionResponseVisualization">
            <h2 class="subtitle has-text-justified">
              Reflection response visualization. The RGB color encodes frequency-dependent reflection response, with <span style="color:red">red</span> indicating <span style="color:red">high-frequency</span> and <span style="color:blue">blue</span> indicating <span style="color:blue">low-frequency</span>. Our method yields diverse, interpretable reflection patterns even with only 0.1% training data. In the middle, we visualize the reflection response curves. The results align with real-world observations 
              -- e.g.,  <span style="color:red"><b>carpet</b></span> exhibits low high-frequency reflectivity, 
              <span style="color:orange"><b>foam</b></span> is generally absorptive, 
              and <span style="color:limegreen"><b>metal</b></span> reflects strongly at high frequencies.
              <!-- Our method realistically simulates room acoustics by integrating visual analysis and acoustic modeling. We first process multi-view images to extract visual features that capture material and surface properties. These visual insights guide acoustic simulations, accurately modeling sound reflections and complex reverberation effects. The entire pipeline is differentiable, allowing end-to-end optimization for enhanced acoustic realism. -->
              <!-- Our framework contains two main components for rendering the room impulse response (RIR): <b>(1) Visual Processing (top):</b> Multi-view images of the scene are passed through a pre-trained vision encoder
              to extract pixel-aligned features at sampled points on the room surface. We then apply cross-attention both across views for each sampled point and across sampled points of query x
              to obtain a unified, material-aware visual feature $\mathrm{F}(\mathbf{x})$ (detailed in Section~\ref{sec:vision}). \textbf{(2) Acoustic Processing (bottom):} On the left, we illustrate our acoustic beam tracing procedure (Section~\ref{sec:beamtracing}), where we sample specular paths and compute the path reflection response, conditioned on both the positional encoding (Section~\ref{sec:mip-rr}) and the visual feature $\mathrm{F}({\bf x})$. On the right, we show how we model the residual acoustic field (Section~\ref{sec:residual}) by treating every point on the surface as a secondary sound source and integrating its contribution via Monte-Carlo integration. The entire pipeline is fully differentiable, enabling end-to-end optimization of both acoustic and visual parameters. -->
              <!-- <math>aa</math> -->
            </h2>
          </figure>
        </div>
      </div>
    </div>
  </div>
</section>




<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">More Comparisons 
      </h2>
      <button
        class="button is-light is-rounded"
        onclick="toggleSection('more-contents', this)">
        <span class="icon">
          <i class="fas fa-chevron-down"></i>
        </span>&nbsp;
      Read More (for people who are interested)
      </button>

      <div class="columns is-centered has-text-centered">
        <div class="column hidden" id="more-contents">
          
          <h2 class="title is-4">Example 1: Piano Chord<sup>[1]</sup></h2>
          <table class="compact-table">
            <tr> 
              <td>Dry Sound:</td>
              <td>
                <audio 
                  controls 
                  src="static/demo-media/furnished-0234/audio.wav">
                </audio>
              </td>
              <td>
                &nbsp; &nbsp; &nbsp; &nbsp; 
              </td>
              <td>Layout:</td>
              <td>
                <img 
                  class="compact-media layout-thumb" 
                  src="static/demo-media/furnished-0234/layout.png" 
                  alt="Layout" />
              </td>
            </tr>
          </table>

          <table class="demo-table">
            <tr>
              <th></th>
              <th>GT</th>
              <th>Ours</th>
              <th>AVR</th>
              <th>INRAS++</th>
            </tr>
            <tr>
              <th>  RIR </th>
              <td><img src="static/demo-media/furnished-0234/gt.png" alt="GT RIR" /></td>
              <td><img src="static/demo-media/furnished-0234/ours.png" alt="Ours RIR" /></td>
              <td><img src="static/demo-media/furnished-0234/avr.png" alt="AVR RIR" /></td>
              <td><img src="static/demo-media/furnished-0234/inras.png" alt="INRAS RIR" /></td>
            </tr>
            <tr>
              <th> Rerverb </th>
              <td><audio controls src="static/demo-media/furnished-0234/gt.wav"></audio></td>
              <td><audio controls src="static/demo-media/furnished-0234/ours.wav"></audio></td>
              <td><audio controls src="static/demo-media/furnished-0234/avr.wav"></audio></td>
              <td><audio controls src="static/demo-media/furnished-0234/inras.wav"></audio></td>
            </tr>
          </table>

          <hr class="is-divider">

          <h2 class="title is-4">Example 2: Stomping Drum<sup>[2]</sup></h2>
          <table class="compact-table">
            <tr> 
              <td>Dry Sound:</td>
              <td>
                <audio 
                  controls 
                  src="static/demo-media/furnished-1792/audio.wav">
                </audio>
              </td>
              <td>
                &nbsp; &nbsp; &nbsp; &nbsp; 
              </td>
              <td>Layout:</td>
              <td>
                <img 
                  class="compact-media layout-thumb" 
                  src="static/demo-media/furnished-1792/layout.png" 
                  alt="Layout" />
              </td>
            </tr>
          </table>

          <table class="demo-table">
            <tr>
              <th></th>
              <th>GT</th>
              <th>Ours</th>
              <th>AVR</th>
              <th>INRAS++</th>
            </tr>
            <tr>
              <th>  RIR </th>
              <td><img src="static/demo-media/furnished-1792/gt.png" alt="GT RIR" /></td>
              <td><img src="static/demo-media/furnished-1792/ours.png" alt="Ours RIR" /></td>
              <td><img src="static/demo-media/furnished-1792/avr.png" alt="AVR RIR" /></td>
              <td><img src="static/demo-media/furnished-1792/inras.png" alt="INRAS RIR" /></td>
            </tr>
            <tr>
              <th> Rerverb </th>
              <td><audio controls src="static/demo-media/furnished-1792/gt.wav"></audio></td>
              <td><audio controls src="static/demo-media/furnished-1792/ours.wav"></audio></td>
              <td><audio controls src="static/demo-media/furnished-1792/avr.wav"></audio></td>
              <td><audio controls src="static/demo-media/furnished-1792/inras.wav"></audio></td>
            </tr>
          </table>


          <!-- <table class="raf-table">
            <thead>
              <tr>
                <th rowspan="3">Method</th>
                <th rowspan="3">Scale</th>
                <th colspan="4">RAF-Empty</th>
                <th colspan="4">RAF-Furnished</th>
              </tr>
              <tr>
                <th>Loudness</th>
                <th>C50</th>
                <th>EDT</th>
                <th>T60</th>
                <th>Loudness</th>
                <th>C50</th>
                <th>EDT</th>
                <th>T60</th>
              </tr>
              <tr>
                <th>(dB) ↓</th>
                <th>(dB) ↓</th>
                <th>(ms) ↓</th>
                <th>(%) ↓</th>
                <th>(dB) ↓</th>
                <th>(dB) ↓</th>
                <th>(ms) ↓</th>
                <th>(%) ↓</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>NAF++</td>
                <td>1%</td>
                <td>6.05</td>
                <td>2.10</td>
                <td>94.5</td>
                <td>23.9</td>
                <td>6.61</td>
                <td>2.10</td>
                <td>74.9</td>
                <td>23.0</td>
              </tr>
              <tr>
                <td>INRAS++ </td>
                <td>1%</td>
                <td>3.69</td>
                <td>2.59</td>
                <td>100.3</td>
                <td>23.5</td>
                <td>2.96</td>
                <td>2.61</td>
                <td>92.6</td>
                <td>25.0</td>
              </tr>
              <tr>
                <td>AV-NeRF</td>
                <td>1%</td>
                <td>3.16</td>
                <td>2.52</td>
                <td>96.4</td>
                <td>21.8</td>
                <td>2.92</td>
                <td>2.64</td>
                <td>96.7</td>
                <td>24.5</td>
              </tr>
              <tr>
                <td>AVR</td>
                <td>1%</td>
                <td class="highlight-secondary">3.00</td>
                <td>2.19</td>
                <td>87.3</td>
                <td>24.1</td>
                <td>2.97</td>
                <td>2.33</td>
                <td class="highlight-secondary">72.3</td>
                <td>17.9</td>
              </tr>
              <tr>
                <td colspan="10" style="border-top: 2px solid #999;"></td>
              </tr>
              <tr>
                <td>Ours</td>
                <td>0.1%</td>
                <td>3.14</td>
                <td class="highlight-secondary">1.81</td>
                <td class="highlight-secondary">86.6</td>
                <td class="highlight-secondary">16.9</td>
                <td class="highlight-secondary">2.45</td>
                <td class="highlight-secondary">1.98</td>
                <td>80.1</td>
                <td class="highlight-secondary">15.2</td>
              </tr>
              <tr>
                <td>Ours</td>
                <td>1%</td>
                <td class="highlight-primary">2.50</td>
                <td class="highlight-primary">1.42</td>
                <td class="highlight-primary">56.2</td>
                <td class="highlight-primary">10.7</td>
                <td class="highlight-primary">1.68</td>
                <td class="highlight-primary">1.29</td>
                <td class="highlight-primary">47.4</td>
                <td class="highlight-primary">9.61</td>
              </tr>
            </tbody>
            <caption>
              Results on the Real Acoustic Field dataset (0.32 s, 16 kHz). Cells highlighted in green denote the best performance, and yellow indicates the second best. Note that our model trained on only 0.1 % of the data already achieves lower C50 and T60 errors than baseline methods, and significantly outperforms all baselines when using the same amount of training data.
            </caption>
          </table> -->

          <!-- <h2 class="title is-5"> For more quantitaive results, please refer to our paper.</h2> -->
          <h2 class="subtitle has-text-justified">
            All the models are trained on 1% of the original training data in the Real Acoustic Field dataset.
            <br>
            Sound sources: [<a href="https://pixabay.com/music/introoutro-stomping-drums-rhythm-and-percussion-141367/" target="_blank">1</a>], [<a href="https://pixabay.com/sound-effects/piano-chord-97902/" target="_blank">2</a>].
          </h2>

        </div>
      </div>
    </div>
  </div>
</section>

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{jin2025avdar,
      title={Differentiable Room Acoustic Rendering with Multi-View Vision Priors}, 
      author={Derong Jin and Ruohan Gao},
      year={2025},
      eprint={2504.21847},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2504.21847}, 
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->
    <!-- <script src="static/js/interactive-demo.js"></script> -->
    <script>
      function toggleSection(divId, btn) {
        const div = document.getElementById(divId);
        if (div.classList.contains('hidden')) {
          div.classList.remove('hidden');
          btn.innerHTML = `
            <span class="icon"><i class="fas fa-chevron-up"></i></span>&nbsp;
            Read Less
          `;
        } else {
          div.classList.add('hidden');
          btn.innerHTML = `
            <span class="icon"><i class="fas fa-chevron-down"></i></span>&nbsp;
            Read More (for people who are interested)
          `;
        }
      }
    </script>
  </body>
  </html>
